{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd04f539",
      "metadata": {},
      "source": [
        "# 1. Importar librerías y configurar entorno\n",
        "En esta sección se definen las librerías y configuraciones básicas necesarias para el resto del notebook. Se incluyen las importaciones de módulos externos y la recuperación de variables de entorno relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a94eaed",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import ast\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain.agents.agent_toolkits import create_retriever_tool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Se asume que la variable de entorno HF_API_KEY ya existe.\n",
        "api_key = os.environ.get(\"HF_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359a17f8",
      "metadata": {},
      "source": [
        "# 2. Conexión con la base de datos\n",
        "Aquí se configura la conexión con la base de datos PostgreSQL mediante `SQLDatabase` de `langchain_community.utilities`. Se utilizan las credenciales para conformar la URI y luego instanciar el objeto `db`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0caeeaf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "usuario = 'postgres'\n",
        "password = 'place_rag_password'\n",
        "host = 'localhost'     # o la IP/URL de tu servidor\n",
        "puerto = '5432'        # puerto por defecto de PostgreSQL\n",
        "base_datos = 'place_rag_db'\n",
        "\n",
        "# Crear la URL de conexión\n",
        "uri = f\"postgresql+psycopg2://{usuario}:{password}@{host}:{puerto}/{base_datos}\"\n",
        "\n",
        "db = SQLDatabase.from_uri(uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbb3bc6",
      "metadata": {},
      "source": [
        "# 3. Definición del Prompt del Sistema\n",
        "Este prompt sirve para guiar al modelo en la generación de consultas SQL, asegurando que solo utilice los nombres de tablas y columnas existentes en el esquema de la base de datos proporcionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f4640bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\n",
        "Dada una pregunta de entrada, crea una consulta de postgresql sintácticamente correcta.\n",
        "Usa solo los nombres de las columnas que puedes ver en la descripción del esquema.\n",
        "No consultes columnas que no existen.\n",
        "Utiliza únicamente las siguientes tablas: 'entidades', 'expedientes', 'paises', 'regiones'\n",
        "Esquema de la base de datos:\n",
        "{db.table_info}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a5c864",
      "metadata": {},
      "source": [
        "# 4. Configurar modelo *Deepseek-coder-1.3b-base*\n",
        "Se configura el modelo de lenguaje *Deepseek-coder* con carga en 8 bits (quantization) para optimizar memoria. También se define el tokenizer correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e9b087",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "# Deshabilitamos el uso de caché para evitar problemas de compatibilidad\n",
        "model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Ajustes del tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61c9b3a",
      "metadata": {},
      "source": [
        "# 5. Definir funciones para generación de consulta y respuesta\n",
        "1. **extraer_query_sql**: Busca dentro del texto la consulta SQL.\n",
        "2. **generar_consulta_deepseek**: Genera una consulta SQL a partir de la pregunta del usuario utilizando el modelo Deepseek.\n",
        "3. **generar_respuesta_deepseek**: Ejecuta la consulta generada y devuelve una respuesta en base a los datos obtenidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e238a8e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extraer_query_sql(texto):\n",
        "    \"\"\"\n",
        "    Busca en el texto una consulta que empiece con SELECT * y termine con ;.\n",
        "    Devuelve la consulta completa si la encuentra.\n",
        "    \"\"\"\n",
        "    patron = re.compile(\n",
        "        r\"SELECT \\*(?:.|\\n)*?;\"\n",
        "    )\n",
        "    consulta = patron.findall(texto)\n",
        "    return consulta\n",
        "\n",
        "def generar_consulta_deepseek(consulta_usuario, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Utiliza el prompt del sistema y la pregunta del usuario para generar una consulta SQL.\n",
        "    Se intenta extraer la consulta final del texto de salida del modelo.\n",
        "    \"\"\"\n",
        "    prompt_text = system_prompt + \" Pregunta: \" + consulta_usuario + \" Comienza la query siempre por SELECT * y termínala siempre por ; Respuesta: SELECT *\"\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    success = False\n",
        "    outputs = None\n",
        "    while not success:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    ai_msg = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        # Intentamos quedarnos con la segunda aparición si hubiera.\n",
        "        resultado = extraer_query_sql(ai_msg)[1]\n",
        "    except:\n",
        "        # Si no la encontramos, devolvemos directamente todo el mensaje.\n",
        "        resultado = ai_msg\n",
        "    return resultado\n",
        "\n",
        "def generar_respuesta_deepseek(query, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Ejecuta la consulta generada contra la base de datos. Con los resultados, construye un prompt\n",
        "    para el modelo Deepseek, que finalmente produce la respuesta al usuario.\n",
        "    \"\"\"\n",
        "    resultado_consulta = db.run(query)\n",
        "    prompt = (\"Responde a la siguiente pregunta\" + consulta_usuario + \n",
        "              \"Utilizando estos datos: \" + str(resultado_consulta) + \n",
        "              \"Si no se aportan datos, responde que no hay contratos que se ajusten a la pregunta.\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    success = False\n",
        "    outputs = None\n",
        "    while not success:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=256)\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    ai_msg = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    try:\n",
        "        # Algunos modelos devuelven la respuesta en .content, etc.\n",
        "        resultado = ai_msg.content\n",
        "    except:\n",
        "        resultado = ai_msg if ai_msg else \"Error inesperado.\"\n",
        "    return resultado"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20fff7c1",
      "metadata": {},
      "source": [
        "# 6. Probar generación de consulta y respuesta con Deepseek\n",
        "En esta sección se ejecuta la función de generación de consulta y, con la consulta resultante, se obtiene la respuesta correspondiente del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7d9a7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generación de consulta\n",
        "consulta_generada = generar_consulta_deepseek(consulta_usuario, model, tokenizer)\n",
        "print(\"Consulta generada:\", consulta_generada)\n",
        "\n",
        "# Generación de respuesta\n",
        "respuesta = generar_respuesta_deepseek(consulta_generada, model, tokenizer)\n",
        "print(\"Respuesta final:\", respuesta)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79fcea8",
      "metadata": {},
      "source": [
        "# 7. Configurar modelo OpenAI GPT-4\n",
        "Se instancia un LLM a través de `ChatOpenAI` apuntando a GPT-4 (simbolizado aquí como `gpt-4o`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c0f6f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    question: str\n",
        "    query: str\n",
        "    result: str\n",
        "    answer: str\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f16832a",
      "metadata": {},
      "source": [
        "# 8. Generar consultas con agente ReAct y GPT-4\n",
        "Se crea un agente tipo ReAct con las herramientas de la base de datos (`SQLDatabaseToolkit`) y el modelo `llm` (GPT-4). El prompt del sistema se comparte para guiar al agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cea71d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "tools = toolkit.get_tools()\n",
        "tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89d7203",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_executor = create_react_agent(llm, tools, prompt=system_prompt)\n",
        "\n",
        "for step in agent_executor.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": consulta_usuario}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b44ceec",
      "metadata": {},
      "source": [
        "# 9. Mejorar la precisión de las consultas mediante *embeddings*\n",
        "Se utiliza la capacidad de embeddings para encontrar variantes de nombres propios y así garantizar consultas más precisas a la base de datos, especialmente al filtrar por localidad, entidad o región."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce910bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_as_list(db, query):\n",
        "    \"\"\"\n",
        "    Ejecuta una consulta y la transforma en una lista con valores únicos,\n",
        "    limpiando valores numéricos y espacios.\n",
        "    \"\"\"\n",
        "    res = db.run(query)\n",
        "    # Convertimos la respuesta en una lista plana\n",
        "    res = [el for sub in ast.literal_eval(str(res)) for el in sub if el]\n",
        "    # Eliminamos posibles números sueltos y limpiamos espacios\n",
        "    res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]\n",
        "    return list(set(res))\n",
        "\n",
        "# Obtenemos listas de entidades y regiones\n",
        "entidades = query_as_list(db, \"SELECT name FROM entidades\")\n",
        "regiones = query_as_list(db, \"SELECT country_subentity_name FROM regiones\")\n",
        "\n",
        "# Creamos un vector store en memoria\n",
        "vector_store = InMemoryVectorStore(OpenAIEmbeddings())\n",
        "\n",
        "# Añadimos textos al vector store\n",
        "vector_store.add_texts(entidades + regiones)\n",
        "\n",
        "# Creamos el retriever\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "description = (\n",
        "    \"Úselo para buscar valores para filtrar. La entrada es una ortografía aproximada \"\n",
        "    \"del nombre propio, la salida son nombres propios válidos. Utilice el sustantivo más \"\n",
        "    \"similar a la búsqueda.\"\n",
        ")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    name=\"search_proper_nouns\",\n",
        "    description=description,\n",
        ")\n",
        "\n",
        "# Sufijo para guiar al sistema en el uso del retriever\n",
        "suffix = (\n",
        "    \"Si necesita filtrar por un nombre propio como el de una entidad o región, SIEMPRE debes buscar primero \"\n",
        "    \"el valor del filtro usando la herramienta 'search_proper_nouns'. No intentes \"\n",
        "    \"adivinar el nombre propio; utiliza esta función para encontrar nombres similares\"\n",
        ")\n",
        "\n",
        "# Unimos prompt del sistema y sufijo\n",
        "system = f\"{system_prompt}\\n\\n{suffix}\"\n",
        "\n",
        "# Añadimos el retriever_tool a la lista de herramientas\n",
        "tools.append(retriever_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80bdad7",
      "metadata": {},
      "source": [
        "# 10. Agente ReAct con embeddings\n",
        "Se crea un nuevo agente ReAct que, además de las herramientas de la base de datos, utiliza la herramienta de recuperación de nombres propios para encontrar la ortografía correcta de entidades o regiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f7d85d",
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = create_react_agent(llm, tools, prompt=system)\n",
        "consulta_usuario = \"¿Tienes información sobre licitaciones publicadas en Vurgos?\"\n",
        "\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": consulta_usuario}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "name": "Documento Reordenado y Documentado"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
